# Week 9 Headstarter Accelerator Project 9 LLM Evaluation Platform

This Google Colab [**Week_9_Headstarter_Accelerator_Project_9_LLM_Evaluation_Platform.ipynb**](https://github.com/AslauAlexandru/Week-9-Headstarter-Accelerator-Project-9-LLM-Evaluation-Platform/blob/main/Week_9_Headstarter_Accelerator_Project_9_LLM_Evaluation_Platform.ipynb) have two parts: 


## Project Statement

LLM Evaluation Platform. For this project, you are tasked with building an evaluation platform where you test to see what combination of system prompts and LLMs work best for your startup's use case.


This project is brought to you by [Nabeel Farooqui]( https://www.linkedin.com/in/nabeel-f/) , 
currently a Software Engineer at Airtable, 
and previously Co-Founder & CTO of Halo Cars, which got acquired by Lyft. 
Nabeel studied Computer Science at the University of Pennsylvania.

For this project, imagine you are an engineer at a startup building AI chat systems. 
You are tasked with developing a platform to evaluate Large Language Models (LLMs) 
for specific tasks. There are many startups that do this today, such as 
[BrainTrust]( https://www.braintrust.dev/), [Arize]( https://arize.com/), and [HumanLoop]( https://humanloop.com/).

Project Requirements:

- Create an full stack web app with an interface for inputting prompts and viewing responses from multiple LLMs side-by-side
- Integrate metrics such as accuracy, relevancy, and response time for each LLM
- Store user prompts and experiment results in a database
- Implement an analytics dashboard for visualizing performance metrics for different prompts and LLMs

## Resources
[LLM Evaluation Guide](
https://www.superannotate.com/blog/llm-evaluation-guide)

[LLM Evaluation Metrics](
https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation)

[LLMs as a Judge](
https://arize.com/blog-course/llm-evaluation-the-definitive-guide/)

[How to Evaluate LLM Performance for Domain-Specific Use Cases](
https://www.youtube.com/watch?v=ZHjulqB-4A0)

[Can Large Language Models Be an Alternative to Human Evaluation?](
https://aclanthology.org/2023.acl-long.870.pdf)












